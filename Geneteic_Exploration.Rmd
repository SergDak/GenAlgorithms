---
title: "Genetic Algoritms - An Exploration"
author: "Sergei Dakov"
date: '2024-07-04'
output: html_document
---


# introduction:

Genetic algorithms are an optimization algorithm utilizing the concepts of evolution and genetics on data.
The main concept is generating a population of viable solutions to a problem, cross breeding them, and allowing the offspring to mutate in the hopes that those hybrid solutions would bring us closer to the optimal solution to the problem.

Genetic algorithms are a more directed application of random search algorithms, and while they may lose out to more refined or greedy algorithms, their main advantage is that they carry very few additional assumptions or requirements.

In this document we will demonstrate the way genetic algorithms solve a problem, and explore the effect different hyper parameters have on the speed and efficiency of finding the optimal solution.
The problem we will explore is finding a target string from randomly generated strings.

# The Algorithm:

step 0: define the problem, and a fitness score with which we will be able to compare solutions and choose the preferred ones.

Step 1: Generate a starting population, this population will serve as a seed for future generations.

Step 2: Select the best performing subset of the population to be the breeding pool for the next generation.

 Step 2.1: Randomly select two members from the current generation.
	
 Step 2.2: Each offspring is generated by randomly selecting elements (genes) from each of the parents.
	
 Step 2.3: Each gene has a chance to randomly mutate into a version that may be different from	both parents.
	
 Step 2.4: Repeat from step 2.1 for desired size of offspring pool.
	

Step 3: Cull the resulting new generation (original breeding pool, plus offspring) back to the size of the original population, using fitness score as criterion.

Step 4: Repeat from step 2 until convergence, or fitness score reaching best possible result.

# The Example:

The problem we will explore in this document is finding a target string from randomly generated strings.
The fitness score in this case will be the amount of characters differing from the target string, giving us a target fitness score of 0.

```{r}
# We will use the tidyverse library to streamline some data manipulation elements
library(tidyverse)

#create target string to discover
target_string <- "hello, this is a secret"

#split the string for easier comparison
target_string <- strsplit(target_string,'',fixed = T)[[1]]

#possible options for random values
candidates <- c(letters,LETTERS,',','.',' ','!','?')
```

We define functions to calculate fitness score, next generation and selection:

```{r}
# calculate fitness score: how many characters differ from target
check_fitness <- function(target,array) {
  sum(target!=array)
}

# select the best subset of population to propogate to next generation
selection <- function(target,population,n_survivors = 50) {
  fitness <- apply(population,MARGIN = 1,FUN = check_fitness,target= target_string)
  survivors <- cbind(population, fitness) %>% arrange(fitness) %>% head(n_survivors)
}

# generate new offspring based on given parents
genetics <- function(mate_1,mate_2,p_mutation = 0.1) {
  offspring <- mate_1
  p_parent <- (1-p_mutation)/2+p_mutation
  roll <- runif(length(mate_1))
  for (i in 1:length(mate_1)) {
    if (roll[i]<= p_mutation) {
      offspring[i] <- sample(candidates,1)
      next
    }
    if (roll[i] <= p_parent){
      offspring[i] <- mate_1[i]
      next
    }
    offspring[i] <- mate_2[i]
  }
  offspring <- data.frame(offspring)
  names(offspring) <- 1:length(mate_1)
  return(offspring)
}

# create next geeration from population
mate <- function(population,n_offspring=3, p_mutation = 0.1) {
  new_generation <- population %>% select(-fitness)
  for(i in 1:(nrow(population)/2)) {
    mate_1 <- population[i,] %>% select(-fitness)
    mate_2 <- sample(1:nrow(population),1)
    mate_2 <- population[mate_2,] %>% select(-fitness)
    for (j in 1:n_offspring){
      new_offspring <- genetics(mate_1,mate_2,p_mutation )
      new_generation <- rbind(new_generation,new_offspring)
    }
  }
  new_generation
}
```

As a baseline we will initiate a starting population with 50 strings, generate 3 offspring per each breeding pair and the mutation probability will be 0.1 (10%) per gene 
We initialie the population at 100 strings, but only te top 50 performing ones are fed into the algorithm

```{r}


#initiate at generation 0
gen_count <- 0
pop_size <- 100

#create starting population
population <- data.frame()
for (i in 1:pop_size) {
  new_member <- sample(candidates,length(target_string),replace = T)
  population <- rbind(population,new_member)
}
names(population) <- 1:length(target_string)
#determine starting fitness score
survivors <- selection(target_string,population)
best_fitness <- survivors %>% head(1) %>% pull(fitness)
# collate number of generations to convergence for effective population of 50, mutation rate of 0.1 and 3 offspring per mating
gens_50_0.1_3 <- c()
#store the initial population for future comparisons
starting_survivors <- survivors
```

Now, we perform the iterative algorithm until we reach the desired fitness score
we can monitor the progress of the algorithm at each iteration, for brevity of output we will display only each 10th iteration

```{r}
set.seed(11)
# repeat until convergence
  while (best_fitness>0) {
    gen_count <- gen_count+1 #increase generation count by 1
    population <- mate(survivors,p_mutation = 0.1) #breed new offspring
    survivors <- selection(target_string,population) #select next generation
    best_fitness <- survivors %>% head(1) %>% pull(fitness)
    best_guess <- survivors %>% head(1) %>% select(-fitness) %>% paste0(.,collapse = '')
    if ((gen_count-1)%%10==0){ #print progress
    print(paste("GENERATION:", gen_count,"VALUE:",best_guess,"FITNESS:", best_fitness))
    }
  }

```

# Exploring the Hyper-Parameters

As we saw on the previous example, the algorithm does reach the desired outcome, but it takes it a large number of iterations to get there.

We can explore the effect of changing some of the algorithm hyper-parameters (population size, number of offspring, mutation chance) to try and improve the performance of the algorithm.
Since the algorithm relies on multiple steps of random generation, we will perform the same task 200 times with each configuration to get a better understanding of average performance.
Also, to make the conditions for each configuration closer to identical, all iterations and configuration will use the same starting population as a seed.

* Note: in this document all results are loaded from a simulation performed outside of this document, running the simulations again may yield different results

## Mutation Chance

One of the main challenges the algorithm showed in the example is that sometimes a certain gene is just not available in the population, and the only way to obtain it is through random mutation (we can see that for example towards the end of the algorithms running, where it struggled to obtain he final character to match the string).
If the mutation probability is low, mutations are rare and rarer still in the "needed" gene so very few of the offspring even have potential to progress the algorithm.
The naive solution to that is to increase the mutation probability, to increase the variance in offspring and make them more likely to it the desired result.

To test that, we perform the same task with mutation probabilities of 0.05,0.15, and 0.2; and compare the results to the initial value of 0.1

```{r, echo=FALSE,message=FALSE}
sim_data <- read.csv('gens_full.csv')
```

We can see the results in the following histogram:

```{r, echo=FALSE}
library(ggplot2)

sim_data %>% select(-contains('population')) %>% select(-contains('offspring')) %>%
  pivot_longer(cols=everything(),names_to = "source", values_to = "value")  %>%
  ggplot() + geom_histogram(aes(x=value,fill=source),binwidth = 25,alpha = 0.4, position = 'identity') +
  theme_light()+
  labs(x="Number of Iterations",y="Count", title = "Histogram of iterations required for the Algorithm", subtitle = "Coloured by Mutation Probability",fill = "P Mutation") +
  scale_fill_manual(labels = c(0.1,0.05,0.15,0.25), values = c("darkred","darkblue","darkgreen","darkorchid")) +
  theme(panel.grid.minor.y = element_blank())
```
```{r, echo=FALSE}
sim_data %>% select(-contains('population')) %>% select(-contains('offspring')) %>%
  pivot_longer(cols=everything(),names_to = "source", values_to = "value")  %>% group_by(source) %>%
  summarize(mean = mean(value),variance = var(value))
```

As We can see, increasing the mutation probability increases the mean number of iterations, as well as the variance in the number of iterations. In fact, for the probability of 0.25 we can see several cases where the number of iterations is in the thousands compared to all other values usually being in the hundreds.

we can observe the effect the mutation probability has on the speed of convergence by plotting the fitness score for a single run of the algorithm in each case:

```{r, echo=FALSE}
starting_pop <- read.csv("starting_pop.csv")
names(starting_pop) <- c(1:length(target_string),'fitness')

fitness_0.1 <- c()
fitness_0.25 <- c()
fitness_0.15 <- c()
fitness_0.05 <- c()

runtimes <- c()

set.seed(14)
best_fitness <- 1
survivors <- starting_pop
begin <- Sys.time()
while (best_fitness>0) {
    population <- mate(survivors)
    survivors <- selection(target_string,population)
    best_fitness <- survivors %>% head(1) %>% pull(fitness)
    fitness_0.1 <- c(fitness_0.1,best_fitness)
}
finish <- Sys.time()
time_itr <- (finish-begin)/length(fitness_0.1)
time_mean <- time_itr*126.965

runtimes <- rbind.data.frame(runtimes,c(time_itr,time_mean))
names(runtimes) = c("per_itr","mean")

best_fitness <- 1
set.seed(14)
survivors <- starting_pop
begin <- Sys.time()
while (best_fitness>0) {
    population <- mate(survivors,p_mutation = 0.25)
    survivors <- selection(target_string,population)
    best_fitness <- survivors %>% head(1) %>% pull(fitness)
    fitness_0.25 <-c(fitness_0.25,best_fitness)
}
finish <- Sys.time()
time_itr <- (finish-begin)/length(fitness_0.25)
time_mean <- time_itr*535.570
runtimes <- rbind.data.frame(runtimes,c("per_itr" = time_itr,"mean" = time_mean))

best_fitness <- 1
set.seed(14)
survivors <- starting_pop
begin <- Sys.time()
while (best_fitness>0) {
    population <- mate(survivors,p_mutation = 0.15)
    survivors <- selection(target_string,population)
    best_fitness <- survivors %>% head(1) %>% pull(fitness)
    fitness_0.15 <-c(fitness_0.15,best_fitness)
}
finish <- Sys.time()
time_itr <- (finish-begin)/length(fitness_0.15)
time_mean <- time_itr*174.590	
runtimes <- rbind.data.frame(runtimes,c("per_itr" = time_itr,"mean" = time_mean))

best_fitness <- 1
set.seed(14)
survivors <- starting_pop
begin <- Sys.time()
while (best_fitness>0) {
    population <- mate(survivors,p_mutation = 0.05)
    survivors <- selection(target_string,population)
    best_fitness <- survivors %>% head(1) %>% pull(fitness)
    fitness_0.05 <-c(fitness_0.05,best_fitness)
}
finish <- Sys.time()
time_itr <- (finish-begin)/length(fitness_0.05)
time_mean <- time_itr*113.125	
runtimes <- rbind.data.frame(runtimes,c("per_itr" = time_itr,"mean" = time_mean))

baseline <- fitness_0.1
fitness_0.1 <- c(fitness_0.1,rep(0,length(fitness_0.25)-length(fitness_0.1)))
fitness_0.05 <- c(fitness_0.05,rep(0,length(fitness_0.25)-length(fitness_0.05)))
fitness_0.15 <- c(fitness_0.15,rep(0,length(fitness_0.25)-length(fitness_0.15)))
fitness_total <- bind_cols('0.1'=fitness_0.1,'0.05' = fitness_0.05,'0.15' = fitness_0.15,'0.25' =fitness_0.25)

fitness_total %>% ggplot() +
  geom_line(aes(x=1:715,y=fitness_0.1,colour="0.1")) +
  geom_line(aes(x=1:715,y=fitness_0.05,colour="0.05")) +
  geom_line(aes(x=1:715,y=fitness_0.15,colour="0.15")) +
  geom_line(aes(x=1:715,y=fitness_0.25,colour="0.25")) +
  theme_light()+
  labs(x="Iteration",y="Fitness", title = "Fitness across Algorithm Iterations", subtitle = "Coloured by Mutation Probability",coulour = "Offspring") +
  scale_colour_manual(labels = c(0.1,0.05,0.15,0.25), values = c("red","forestgreen","Blue","purple")) +
  theme(panel.grid.minor.y = element_blank())
```

We can see that while generally, higher mutation probability causes longer convergence, in this instance p=0.15 performed the best.
Also, p=0.25 struggled significantly more wit matching the last few characters. It is possible that while the higher mutation probability increases the chances for the last "wrong" genes to be corrected, other correct genes mutate away from the solution, providing no progress in aggregate.

## Number of Offspring

Another way of increasing the odds of a generation improving on its predecessor is increasing the number of offspring.
with an increase in the number of offspring, there are more opportunities for a mutation to occur on the correct gene, while keeping the overall mutation count low

For comparison we will test 1,5, and 10 offspring per pairing, with 3 being the baseline

```{r, echo=F}

sim_data %>% select(-contains('population')) %>% select(-contains('mutation')) %>%
  pivot_longer(cols=everything(),names_to = "source", values_to = "value")  %>%
  ggplot() + geom_histogram(aes(x=value,fill=source),binwidth = 25,alpha = 0.4, position = 'identity') +
    theme_light()+
  labs(x="Number of Iterations",y="Count", title = "Histogram of iterations required for the Algorithm", subtitle = "Coloured by Number of Offspring",fill = "Offspring") +
  scale_fill_manual(labels = c(3,1,10,5), values = c("blue3","red3","green3","purple3")) +
  theme(panel.grid.minor.y = element_blank())
```

```{r echo=FALSE}
sim_data %>% select(-contains('population')) %>% select(-contains('mutation')) %>%
  pivot_longer(cols=everything(),names_to = "source", values_to = "value")  %>% group_by(source) %>%
  summarize(mean = mean(value),variance = var(value))
```

We can see from the data that increasing the number of offspring not only lowers the amount of iterations required, but does it more consistently (lower variance)

for the convergence rate we get:

```{r, echo=FALSE}
offspring_1 <- c()
offspring_5 <- c()
offspring_10 <- c()

set.seed(14)
best_fitness <- 1
survivors <- starting_pop
begin <- Sys.time()
while (best_fitness>0) {
    population <- mate(survivors,n_offspring = 1)
    survivors <- selection(target_string,population)
    best_fitness <- survivors %>% head(1) %>% pull(fitness)
    offspring_1 <- c(offspring_1,best_fitness)
}
finish <- Sys.time()
time_itr <- (finish-begin)/length(offspring_1)
time_mean <- time_itr*379.195
runtimes <- rbind.data.frame(runtimes,c("per_itr" = time_itr,"mean" = time_mean))

best_fitness <- 1
set.seed(14)
survivors <- starting_pop
begin <- Sys.time()
while (best_fitness>0) {
    population <- mate(survivors,n_offspring = 10)
    survivors <- selection(target_string,population)
    best_fitness <- survivors %>% head(1) %>% pull(fitness)
    offspring_10 <-c(offspring_10,best_fitness)
}
finish <- Sys.time()
time_itr <- (finish-begin)/length(offspring_10)
time_mean <- time_itr*44.735	
runtimes <- rbind.data.frame(runtimes,c("per_itr" = time_itr,"mean" = time_mean))

best_fitness <- 1
set.seed(14)
survivors <- starting_pop
begin <- Sys.time()
while (best_fitness>0) {
    population <- mate(survivors,n_offspring = 5)
    survivors <- selection(target_string,population)
    best_fitness <- survivors %>% head(1) %>% pull(fitness)
    offspring_5 <-c(offspring_5,best_fitness)
}
finish <- Sys.time()
time_itr <- (finish-begin)/length(offspring_5)
time_mean <- time_itr*84.125	
runtimes <- rbind.data.frame(runtimes,c("per_itr" = time_itr,"mean" = time_mean))


offspring_3 <- c(baseline,rep(0,length(offspring_1)-length(baseline)))
offspring_5 <- c(offspring_5,rep(0,length(offspring_1)-length(offspring_5)))
offspring_10 <- c(offspring_10,rep(0,length(offspring_1)-length(offspring_10)))
fitness_total <- bind_cols('1'=offspring_1,'3' = offspring_3,'5' = offspring_5,'10' =offspring_10)

fitness_total %>% ggplot() +
  geom_line(aes(x=1:1013,y=offspring_3,colour="3 offspring")) +
  geom_line(aes(x=1:1013,y=offspring_5,colour="5 offspring")) +
  geom_line(aes(x=1:1013,y=offspring_1,colour="1 offspring")) +
  geom_line(aes(x=1:1013,y=offspring_10,colour="10 offspring"))+
  theme_light()+
  labs(x="Iteration",y="Fitness", title = "Fitness across Algorithm Iterations", subtitle = "Coloured by Number of Offspring",coulour = "Offspring") +
  scale_colour_manual(labels = c(3,1,10,5), values = c("red","forestgreen","Blue","purple")) +
  theme(panel.grid.minor.y = element_blank())
```

The above plot shows that increasing the number of offspring indeed increases convergence speed, especially in the final stages that often rely on mutations exclusively.
in this particular case, changing the number of offspring to 10 has shortened the number of iterations from 179 to 28, which is a significant decrease

## Population size

Another way to deal with the limitations of the gene pool is increasing the size of the population, this helps since in the case of discrete fitness scores there is the possibility of multiple ties causing the cutoff point to be arbitrary and possibly losing "good" genes tat could benefit future generations. This as the added benefit of  increasing the number of offspring, since the amount of pairs made depends on the size of the population (the size of the breeding population is a set fraction of the general population).
to test the effect of population size we will test for population sizes of 100,150, and 200 

```{r, echo=F}
sim_data %>% select(-contains('offspring')) %>% select(-contains('mutation')) %>%
  pivot_longer(cols=everything(),names_to = "source", values_to = "value")  %>%
  ggplot() + geom_histogram(aes(x=value,fill=source),binwidth = 25,alpha = 0.4, position = 'identity') +
    theme_light()+
  labs(x="Number of Iterations",y="Count", title = "Histogram of iterations Required for the Algorithm", subtitle = "Coloured by Population Size",fill = "Population:") +
  scale_fill_manual(labels = c(50,100,150,200), values = c("blue","green","orange","purple")) +
  theme(panel.grid.minor.y = element_blank())
```

```{r, echo =F}
sim_data %>% select(-contains('offspring')) %>% select(-contains('mutation')) %>%
  pivot_longer(cols=everything(),names_to = "source", values_to = "value")  %>% group_by(source) %>%
  summarize(mean = mean(value),variance = var(value))
```

True to the prediction, increasing the population size does improve the algorithms performance, and lowers variance.
we can observe the convergence rate:

```{r, echo=F}
population_100 <- c()
population_150 <- c()
population_200 <- c()

set.seed(14)
best_fitness <- 1
survivors <- starting_pop
begin <- Sys.time()
while (best_fitness>0) {
    population <- mate(survivors)
    survivors <- selection(target_string,population,n_survivors = 100)
    best_fitness <- survivors %>% head(1) %>% pull(fitness)
    population_100 <- c(population_100,best_fitness)
}
finish <- Sys.time()
time_itr <- (finish-begin)/length(population_100)
time_mean <- time_itr*58.350	
runtimes <- rbind.data.frame(runtimes,c("per_itr" = time_itr,"mean" = time_mean))

best_fitness <- 1
set.seed(14)
survivors <- starting_pop
begin <- Sys.time()
while (best_fitness>0) {
    population <- mate(survivors)
    survivors <- selection(target_string,population,n_survivors = 150)
    best_fitness <- survivors %>% head(1) %>% pull(fitness)
    population_150 <-c(population_150,best_fitness)
}
finish <- Sys.time()
time_itr <- (finish-begin)/length(population_150)
time_mean <- time_itr*40.235	
runtimes <- rbind.data.frame(runtimes,c("per_itr" = time_itr,"mean" = time_mean))

best_fitness <- 1
set.seed(14)
survivors <- starting_pop
begin <- Sys.time()
while (best_fitness>0) {
    population <- mate(survivors)
    survivors <- selection(target_string,population,n_survivors = 200)
    best_fitness <- survivors %>% head(1) %>% pull(fitness)
    population_200 <-c(population_200,best_fitness)
}
finish <- Sys.time()
time_itr <- (finish-begin)/length(population_200)
time_mean <- time_itr*33.975
runtimes <- rbind.data.frame(runtimes,c("per_itr" = time_itr,"mean" = time_mean))


population_100 <- c(population_100,rep(0,length(baseline)-length(population_100)))
population_150 <- c(population_150,rep(0,length(baseline)-length(population_150)))
population_200 <-c(population_200,rep(0,length(baseline)-length(population_200)))
population_total <- bind_cols('50'=baseline,'100' = population_100,'150' = population_150,'200' = population_200)

population_total %>% ggplot() +
  geom_line(aes(x=1:179,y=baseline,colour="50 population")) +
  geom_line(aes(x=1:179,y=population_100,colour="100 population")) +
  geom_line(aes(x=1:179,y=population_150,colour="150 population")) +
  geom_line(aes(x=1:179,y=population_200,colour="200 population")) +
  theme_light()+
  labs(x="Iteration",y="Fitness", title = "Fitness across Algorithm Iterations", subtitle = "Coloured by Population Size",coulour = "Population") +
  scale_colour_manual(labels = c(50,100,150,200), values = c("red","forestgreen","Blue","purple")) +
  theme(panel.grid.minor.y = element_blank())
```

In this case all populations above the baseline performed similarly, all achieving sub 50 iteration amounts; once again, mostly due to shortening the final stages.

#Time Concerns

So far, we found optimal hyper parameter values to minimize the number of iterations required to complete the task, but increasing the size of the population or the number of offspring also increases the complexity of the model thus making each iteration take longer to perform.
In the real world, we rarely care a bout the number of iterations it takes to get the results, rather we care about how quick we can get those results.
we can take some of the best performing solutions we calculated so far and use system time calculations (or any benchmarking library) to calculate time taken per iteration. Using that time estimate we can calculate the average time it took to complete the task for each configuration:

```{r, echo=F}
configuration <- c("baseline","p mutation = 0.05","p mutation = 0.15","p mutation = 0.25","n offspring = 1", "n offspring = 10", "n offspring = 5", " n population = 100", "n population = 150", "n population = 200")

runtimes <- bind_cols(runtimes,"configuration" = configuration)
```

```{r, echo=FALSE}
runtimes %>% select(-mean) %>% arrange(per_itr)

```
note - the times displayed are in minutes
On a per iteration basis we can see that the simpler versions of the algorithm perform faster, in some cases significantly so.
We can compare the average time taken by multiplying the time per iteration by the average number of iterations for each configuration:

```{r, echo=F}
runtimes %>% arrange(mean)
```

overall, we can see that the simpler models tend to  perform better. we can attempt to combine the better performing configurations together to push the algorithm even further.

Specifically we will try the configurations of: population size 200, mutation chance 0.05 and 1 offspring; and poulation size 50, mutation chance of 0.05, and 1 offspring

```{r, echo=FALSE}

sim_data <- read.csv("gens_mixed.csv")

sim_data %>% select(-contains('offspring')) %>% select(-contains('mutation')) %>% select(-contains('population')) %>%
  pivot_longer(cols=everything(),names_to = "source", values_to = "value")  %>%
  ggplot() + geom_histogram(aes(x=value,fill=source),binwidth = 25,alpha = 0.4, position = 'identity') +
    theme_light()+
  labs(x="Number of Iterations",y="Count", title = "Histogram of iterations Required for the Algorithm", subtitle = "For varying N-population P-mutation and M-offspring",fill = "Model") +
  scale_fill_manual(labels = c("n=200 p=0.05 m=1","n=50 p=0.05 m=1","Baseline"), values = c("blue","red","green")) +
  theme(panel.grid.minor.y = element_blank())

sim_data %>% select(-contains('offspring')) %>% select(-contains('mutation'))  %>% select(-contains('population')) %>%
  pivot_longer(cols=everything(),names_to = "source", values_to = "value")  %>% group_by(source) %>%
  summarize(mean = mean(value),variance = var(value)) %>% arrange(mean)
```

Both complex models complete the task in fewer iterations than the baseline, as well as more consistently, in terms of average completion time:

```{r, echo=FALSE}
times_big <- read.csv("times_mixed.csv")

times_big %>%
  pivot_longer(cols=everything(),names_to = "source", values_to = "value")  %>%
  ggplot() + geom_histogram(aes(x=value,fill=source),binwidth = 25,alpha = 0.4, position = 'identity') +
    theme_light()+
  labs(x="Seconds",y="Count", title = "Histogram of Completion Times in seconds", subtitle = "For varying N-population P-mutation and M-offspring",fill = "Model") +
  scale_fill_manual(labels = c("n=200 p=0.05 m=1","n=50 p=0.05 m=1","Baseline"), values = c("blue","red","green")) +
  theme(panel.grid.minor.y = element_blank())

times_big %>% 
  pivot_longer(cols=everything(),names_to = "source", values_to = "value")  %>% group_by(source) %>%
  summarize(mean = mean(value),variance = var(value)) %>% arrange(mean)
```

Despite the larger population being faster in terms of iterations, each iteration takes significantly longer to complete, making this configuration slower in the long run

It is hard to say that one configuration is significantly better than the others as the variance is large, and thus it is hard to predict which configuration will perform better at any given time

